{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMIV5UkfDgJX"
      },
      "outputs": [],
      "source": [
        "#Install libraries\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Wandb - Create and login to https://wandb.ai/ and paste the access token\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "DPizFHvcjDbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "GxC-j6ezD-Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset from hugging face - https://huggingface.co/datasets/demelin/moral_stories\n",
        "dataset=load_dataset('demelin/moral_stories','full')"
      ],
      "metadata": {
        "id": "YRWZPf0eriBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = dataset['train']['norm'][:8000]\n",
        "X_test = dataset['train']['norm'][-2000:]\n",
        "print(\"Total Dataset (Including Validation) - \", len(dataset['train']))\n",
        "print(\"Train Dataset - \",len(X_train),'\\n',\"Test Dataset - \",len(X_test))"
      ],
      "metadata": {
        "id": "DJOlmDx6FgdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create GPT2 Tokenizer and define model"
      ],
      "metadata": {
        "id": "xGCeIcwMmJS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refer Hugging face documentation on Transformer https://huggingface.co/docs/transformers/v4.21.2/\n",
        "'''<<Insert tokenizer, model and resize token embedding code here>>'''"
      ],
      "metadata": {
        "id": "f9mNrpLmsE0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize data and split dataset into training and validation"
      ],
      "metadata": {
        "id": "aongHStfnxH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(tokenizer.encode(x)) for x in X_train])"
      ],
      "metadata": {
        "id": "5s4SbSvcrM9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class moral():\n",
        "    def __init__(self, x, tokenizer, max_length):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for txt in x:\n",
        "            encodings_dict = '''<<Insert tokenizer code and keep max_length as defined in above cell>>'''\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n"
      ],
      "metadata": {
        "id": "Z_V2C1e3q42D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data into 80-20 to training and validation \n",
        "dataset = moral(X_train, tokenizer, max_length=max_length)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
      ],
      "metadata": {
        "id": "SohdfTumrvS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training parameters"
      ],
      "metadata": {
        "id": "_z0KuJFDoFdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refer wandb documentation to log training https://docs.wandb.ai/guides/integrations/huggingface\n",
        "training_args='''<<Insert code here for training arguments and experiment with the hyperparameters mentioned in the read me. Leave the rest to their default values.>>'''"
      ],
      "metadata": {
        "id": "wYQPvQbzoOrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_WATCH=all\n",
        "%env WANDB_SILENT=true"
      ],
      "metadata": {
        "id": "oDrf5Hu6t1ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "MIuCNmpeoQJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''<<Insert code for trainer here>>'''"
      ],
      "metadata": {
        "id": "yzGEoXwjuKYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "L-rTbW0EtxzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference using sample data\n"
      ],
      "metadata": {
        "id": "I-KFsT3KpPzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''<<Insert code for Tokenize sample data and use model.generate function to get predicted logits and decode them using tokenizer.decode function refer hugging face documentation for further information >>'''"
      ],
      "metadata": {
        "id": "jjQIrNujF4z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run inference on test dataset"
      ],
      "metadata": {
        "id": "Ie7ZPX_dpdwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''<<Insert code for infering every sentence in test dataset X_test >>'''"
      ],
      "metadata": {
        "id": "HBeI5hjD5Igr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model using BLEU score"
      ],
      "metadata": {
        "id": "6srry5VhpxFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare predicted sentence from test data with actual test data refer NLTK documentation for sentence_blue\n",
        "import statistics\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "m3pZgu-yLiA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
